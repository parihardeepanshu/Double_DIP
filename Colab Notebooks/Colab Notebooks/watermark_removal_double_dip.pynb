{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"watermark_removal_double_dip.pynb","provenance":[{"file_id":"1kcmcFXvxd9Vcf7UcmLvNMjTLxAp5O-9R","timestamp":1586532230897}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3TIx-Nr9kh7d","colab_type":"code","outputId":"baabd552-729d-4f00-a132-9d1dd34f398c","executionInfo":{"status":"ok","timestamp":1586827632516,"user_tz":240,"elapsed":1654,"user":{"displayName":"Deepanshu Parihar","photoUrl":"","userId":"16330078291469187161"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GisxZb-3t6LW","colab_type":"code","colab":{}},"source":["from collections import namedtuple\n","from skimage.measure import compare_psnr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lTZL-ZitzJ-","colab_type":"code","colab":{}},"source":["# image_io.py\n","import glob\n","import torch\n","import torchvision\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","# import skvideo.io\n","\n","output_path_str = \"/content/drive/My Drive/watermark/op_text_4000/\"\n","matplotlib.use('agg')\n","\n","\n","def crop_image(img, d=32):\n","    \"\"\"\n","    Make dimensions divisible by d\n","    :param pil img:\n","    :param d:\n","    :return:\n","    \"\"\"\n","\n","    new_size = (img.size[0] - img.size[0] % d,\n","                img.size[1] - img.size[1] % d)\n","\n","    bbox = [\n","        int((img.size[0] - new_size[0]) / 2),\n","        int((img.size[1] - new_size[1]) / 2),\n","        int((img.size[0] + new_size[0]) / 2),\n","        int((img.size[1] + new_size[1]) / 2),\n","    ]\n","\n","    img_cropped = img.crop(bbox)\n","    return img_cropped\n","\n","\n","def crop_np_image(img_np, d=32):\n","    return torch_to_np(crop_torch_image(np_to_torch(img_np), d))\n","\n","\n","def crop_torch_image(img, d=32):\n","    \"\"\"\n","    Make dimensions divisible by d\n","    image is [1, 3, W, H] or [3, W, H]\n","    :param pil img:\n","    :param d:\n","    :return:\n","    \"\"\"\n","    new_size = (img.shape[-2] - img.shape[-2] % d,\n","                img.shape[-1] - img.shape[-1] % d)\n","    pad = ((img.shape[-2] - new_size[-2]) // 2, (img.shape[-1] - new_size[-1]) // 2)\n","\n","    if len(img.shape) == 4:\n","        return img[:, :, pad[-2]: pad[-2] + new_size[-2], pad[-1]: pad[-1] + new_size[-1]]\n","    assert len(img.shape) == 3\n","    return img[:, pad[-2]: pad[-2] + new_size[-2], pad[-1]: pad[-1] + new_size[-1]]\n","\n","\n","def get_params(opt_over, net, net_input, downsampler=None):\n","    \"\"\"\n","    Returns parameters that we want to optimize over.\n","    :param opt_over: comma separated list, e.g. \"net,input\" or \"net\"\n","    :param net: network\n","    :param net_input: torch.Tensor that stores input `z`\n","    :param downsampler:\n","    :return:\n","    \"\"\"\n","\n","    opt_over_list = opt_over.split(',')\n","    params = []\n","\n","    for opt in opt_over_list:\n","\n","        if opt == 'net':\n","            params += [x for x in net.parameters()]\n","        elif opt == 'down':\n","            assert downsampler is not None\n","            params = [x for x in downsampler.parameters()]\n","        elif opt == 'input':\n","            net_input.requires_grad = True\n","            params += [net_input]\n","        else:\n","            assert False, 'what is it?'\n","\n","    return params\n","\n","\n","def get_image_grid(images_np, nrow=8):\n","    \"\"\"\n","    Creates a grid from a list of images by concatenating them.\n","    :param images_np:\n","    :param nrow:\n","    :return:\n","    \"\"\"\n","    images_torch = [torch.from_numpy(x).type(torch.FloatTensor) for x in images_np]\n","    torch_grid = torchvision.utils.make_grid(images_torch, nrow)\n","\n","    return torch_grid.numpy()\n","\n","\n","def plot_image_grid(name, images_np, interpolation='lanczos', output_path=output_path_str):\n","    \"\"\"\n","    Draws images in a grid\n","    Args:\n","        images_np: list of images, each image is np.array of size 3xHxW or 1xHxW\n","        nrow: how many images will be in one row\n","        interpolation: interpolation used in plt.imshow\n","    \"\"\"\n","    assert len(images_np) == 2 \n","    n_channels = max(x.shape[0] for x in images_np)\n","    assert (n_channels == 3) or (n_channels == 1), \"images should have 1 or 3 channels\"\n","\n","    images_np = [x if (x.shape[0] == n_channels) else np.concatenate([x, x, x], axis=0) for x in images_np]\n","\n","    grid = get_image_grid(images_np, 2)\n","\n","    if images_np[0].shape[0] == 1:\n","        plt.imshow(grid[0], cmap='gray', interpolation=interpolation)\n","    else:\n","        plt.imshow(grid.transpose(1, 2, 0), interpolation=interpolation)\n","\n","    plt.savefig(output_path + \"{}.png\".format(name))\n","\n","\n","def save_image(name, image_np, output_path=output_path_str):\n","    p = np_to_pil(image_np)\n","    p.save(output_path + \"{}.jpg\".format(name))\n","\n","def video_to_images(file_name, name):\n","    video = prepare_video(file_name)\n","    for i, f in enumerate(video):\n","        save_image(name + \"_{0:03d}\".format(i), f)\n","\n","def images_to_video(images_dir ,name, gray=True):\n","    num = len(glob.glob(images_dir +\"/*.jpg\"))\n","    c = []\n","    for i in range(num):\n","        if gray:\n","            img = prepare_gray_image(images_dir + \"/\"+  name +\"_{}.jpg\".format(i))\n","        else:\n","            img = prepare_image(images_dir + \"/\"+name+\"_{}.jpg\".format(i))\n","        print(img.shape)\n","        c.append(img)\n","    save_video(name, np.array(c))\n","\n","def save_heatmap(name, image_np):\n","    cmap = plt.get_cmap('jet')\n","\n","    rgba_img = cmap(image_np)\n","    rgb_img = np.delete(rgba_img, 3, 2)\n","    save_image(name, rgb_img.transpose(2, 0, 1))\n","\n","\n","def save_graph(name, graph_list, output_path=output_path_str):\n","    plt.clf()\n","    plt.plot(graph_list)\n","    plt.savefig(output_path + name + \".png\")\n","\n","\n","def create_augmentations(np_image):\n","    \"\"\"\n","    convention: original, left, upside-down, right, rot1, rot2, rot3\n","    :param np_image:\n","    :return:\n","    \"\"\"\n","    aug = [np_image.copy(), np.rot90(np_image, 1, (1, 2)).copy(),\n","           np.rot90(np_image, 2, (1, 2)).copy(), np.rot90(np_image, 3, (1, 2)).copy()]\n","    flipped = np_image[:,::-1, :].copy()\n","    aug += [flipped.copy(), np.rot90(flipped, 1, (1, 2)).copy(), np.rot90(flipped, 2, (1, 2)).copy(), np.rot90(flipped, 3, (1, 2)).copy()]\n","    return aug\n","\n","\n","def create_video_augmentations(np_video):\n","    \"\"\"\n","        convention: original, left, upside-down, right, rot1, rot2, rot3\n","        :param np_video:\n","        :return:\n","        \"\"\"\n","    aug = [np_video.copy(), np.rot90(np_video, 1, (2, 3)).copy(),\n","           np.rot90(np_video, 2, (2, 3)).copy(), np.rot90(np_video, 3, (2, 3)).copy()]\n","    flipped = np_video[:, :, ::-1, :].copy()\n","    aug += [flipped.copy(), np.rot90(flipped, 1, (2, 3)).copy(), np.rot90(flipped, 2, (2, 3)).copy(),\n","            np.rot90(flipped, 3, (2, 3)).copy()]\n","    return aug\n","\n","\n","def save_graphs(name, graph_dict, output_path=output_path_str):\n","    \"\"\"\n","    :param name:\n","    :param dict graph_dict: a dict from the name of the list to the list itself.\n","    :return:\n","    \"\"\"\n","    plt.clf()\n","    fig, ax = plt.subplots()\n","    for k, v in graph_dict.items():\n","        ax.plot(v, label=k)\n","        # ax.semilogy(v, label=k)\n","    ax.set_xlabel('iterations')\n","    # ax.set_ylabel(name)\n","    ax.set_ylabel('MSE-loss')\n","    # ax.set_ylabel('PSNR')\n","    plt.legend()\n","    plt.savefig(output_path + name + \".png\")\n","\n","\n","def load(path):\n","    \"\"\"Load PIL image.\"\"\"\n","    img = Image.open(path)\n","    return img\n","\n","\n","def get_image(path, imsize=-1):\n","    \"\"\"Load an image and resize to a cpecific size.\n","    Args:\n","        path: path to image\n","        imsize: tuple or scalar with dimensions; -1 for `no resize`\n","    \"\"\"\n","    img = load(path)\n","\n","    if isinstance(imsize, int):\n","        imsize = (imsize, imsize)\n","\n","    if imsize[0] != -1 and img.size != imsize:\n","        if imsize[0] > img.size[0]:\n","            img = img.resize(imsize, Image.BICUBIC)\n","        else:\n","            img = img.resize(imsize, Image.ANTIALIAS)\n","\n","    img_np = pil_to_np(img)\n","\n","    return img, img_np\n","\n","\n","def prepare_image(file_name):\n","    \"\"\"\n","    loads makes it divisible\n","    :param file_name:\n","    :return: the numpy representation of the image\n","    \"\"\"\n","    img_pil = crop_image(get_image(file_name, -1)[0], d=32)\n","    return pil_to_np(img_pil)\n","\n","\n","def prepare_video(file_name, folder=\"output/\"):\n","    data = skvideo.io.vread(folder + file_name)\n","    return crop_torch_image(data.transpose(0, 3, 1, 2).astype(np.float32) / 255.)[:35]\n","\n","\n","def save_video(name, video_np, output_path=output_path_str):\n","    outputdata = video_np * 255\n","    outputdata = outputdata.astype(np.uint8)\n","    skvideo.io.vwrite(output_path + \"{}.mp4\".format(name), outputdata.transpose(0, 2, 3, 1))\n","\n","\n","def prepare_gray_image(file_name):\n","    img = prepare_image(file_name)\n","    return np.array([np.mean(img, axis=0)])\n","\n","\n","def pil_to_np(img_PIL, with_transpose=True):\n","    \"\"\"\n","    Converts image in PIL format to np.array.\n","    From W x H x C [0...255] to C x W x H [0..1]\n","    \"\"\"\n","    ar = np.array(img_PIL)\n","    if len(ar.shape) == 3 and ar.shape[-1] == 4:\n","        ar = ar[:, :, :3]\n","        # this is alpha channel\n","    if with_transpose:\n","        if len(ar.shape) == 3:\n","            ar = ar.transpose(2, 0, 1)\n","        else:\n","            ar = ar[None, ...]\n","\n","    return ar.astype(np.float32) / 255.\n","\n","\n","def median(img_np_list):\n","    \"\"\"\n","    assumes C x W x H [0..1]\n","    :param img_np_list:\n","    :return:\n","    \"\"\"\n","    assert len(img_np_list) > 0\n","    l = len(img_np_list)\n","    shape = img_np_list[0].shape\n","    result = np.zeros(shape)\n","    for c in range(shape[0]):\n","        for w in range(shape[1]):\n","            for h in range(shape[2]):\n","                result[c, w, h] = sorted(i[c, w, h] for i in img_np_list)[l//2]\n","    return result\n","\n","\n","def average(img_np_list):\n","    \"\"\"\n","    assumes C x W x H [0..1]\n","    :param img_np_list:\n","    :return:\n","    \"\"\"\n","    assert len(img_np_list) > 0\n","    l = len(img_np_list)\n","    shape = img_np_list[0].shape\n","    result = np.zeros(shape)\n","    for i in img_np_list:\n","        result += i\n","    return result / l\n","\n","\n","def np_to_pil(img_np):\n","    \"\"\"\n","    Converts image in np.array format to PIL image.\n","    From C x W x H [0..1] to  W x H x C [0...255]\n","    :param img_np:\n","    :return:\n","    \"\"\"\n","    ar = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n","\n","    if img_np.shape[0] == 1:\n","        ar = ar[0]\n","    else:\n","        assert img_np.shape[0] == 3, img_np.shape\n","        ar = ar.transpose(1, 2, 0)\n","\n","    return Image.fromarray(ar)\n","\n","\n","def np_to_torch(img_np):\n","    \"\"\"\n","    Converts image in numpy.array to torch.Tensor.\n","    From C x W x H [0..1] to  C x W x H [0..1]\n","    :param img_np:\n","    :return:\n","    \"\"\"\n","    return torch.from_numpy(img_np)[None, :]\n","\n","\n","def torch_to_np(img_var):\n","    \"\"\"\n","    Converts an image in torch.Tensor format to np.array.\n","    From 1 x C x W x H [0..1] to  C x W x H [0..1]\n","    :param img_var:\n","    :return:\n","    \"\"\"\n","    return img_var.detach().cpu().numpy()[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLMdyEThuRR3","colab_type":"code","colab":{}},"source":["# imresize.py\n","import numpy as np\n","from scipy.ndimage import filters, measurements, interpolation\n","from math import pi\n","\n","\n","def imresize(im, scale_factor=None, output_shape=None, kernel=None, antialiasing=True, kernel_shift_flag=False):\n","    # First standardize values and fill missing arguments (if needed) by deriving scale from output shape or vice versa\n","    scale_factor, output_shape = fix_scale_and_size(im.shape, output_shape, scale_factor)\n","\n","    # For a given numeric kernel case, just do convolution and sub-sampling (downscaling only)\n","    if type(kernel) == np.ndarray and scale_factor[0] <= 1:\n","        return numeric_kernel(im, kernel, scale_factor, output_shape, kernel_shift_flag)\n","\n","    # Choose interpolation method, each method has the matching kernel size\n","    method, kernel_width = {\n","        \"cubic\": (cubic, 4.0),\n","        \"lanczos2\": (lanczos2, 4.0),\n","        \"lanczos3\": (lanczos3, 6.0),\n","        \"box\": (box, 1.0),\n","        \"linear\": (linear, 2.0),\n","        None: (cubic, 4.0)  # set default interpolation method as cubic\n","    }.get(kernel)\n","\n","    # Antialiasing is only used when downscaling\n","    antialiasing *= (scale_factor[0] < 1)\n","\n","    # Sort indices of dimensions according to scale of each dimension. since we are going dim by dim this is efficient\n","    sorted_dims = np.argsort(np.array(scale_factor)).tolist()\n","\n","    # Iterate over dimensions to calculate local weights for resizing and resize each time in one direction\n","    out_im = np.copy(im)\n","    for dim in sorted_dims:\n","        # No point doing calculations for scale-factor 1. nothing will happen anyway\n","        if scale_factor[dim] == 1.0:\n","            continue\n","\n","        # for each coordinate (along 1 dim), calculate which coordinates in the input image affect its result and the\n","        # weights that multiply the values there to get its result.\n","        weights, field_of_view = contributions(im.shape[dim], output_shape[dim], scale_factor[dim],\n","                                               method, kernel_width, antialiasing)\n","\n","        # Use the affecting position values and the set of weights to calculate the result of resizing along this 1 dim\n","        out_im = resize_along_dim(out_im, dim, weights, field_of_view)\n","\n","    return out_im\n","\n","\n","def fix_scale_and_size(input_shape, output_shape, scale_factor):\n","    # First fixing the scale-factor (if given) to be standardized the function expects (a list of scale factors in the\n","    # same size as the number of input dimensions)\n","    if scale_factor is not None:\n","        # By default, if scale-factor is a scalar we assume 2d resizing and duplicate it.\n","        if np.isscalar(scale_factor):\n","            scale_factor = [scale_factor, scale_factor]\n","\n","        # We extend the size of scale-factor list to the size of the input by assigning 1 to all the unspecified scales\n","        scale_factor = list(scale_factor)\n","        scale_factor.extend([1] * (len(input_shape) - len(scale_factor)))\n","\n","    # Fixing output-shape (if given): extending it to the size of the input-shape, by assigning the original input-size\n","    # to all the unspecified dimensions\n","    if output_shape is not None:\n","        output_shape = list(np.uint(np.array(output_shape))) + list(input_shape[len(output_shape):])\n","\n","    # Dealing with the case of non-give scale-factor, calculating according to output-shape. note that this is\n","    # sub-optimal, because there can be different scales to the same output-shape.\n","    if scale_factor is None:\n","        scale_factor = 1.0 * np.array(output_shape) / np.array(input_shape)\n","\n","    # Dealing with missing output-shape. calculating according to scale-factor\n","    if output_shape is None:\n","        output_shape = np.uint(np.ceil(np.array(input_shape) * np.array(scale_factor)))\n","\n","    return scale_factor, output_shape\n","\n","\n","def contributions(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n","    # This function calculates a set of 'filters' and a set of field_of_view that will later on be applied\n","    # such that each position from the field_of_view will be multiplied with a matching filter from the\n","    # 'weights' based on the interpolation method and the distance of the sub-pixel location from the pixel centers\n","    # around it. This is only done for one dimension of the image.\n","\n","    # When anti-aliasing is activated (default and only for downscaling) the receptive field is stretched to size of\n","    # 1/sf. this means filtering is more 'low-pass filter'.\n","    fixed_kernel = (lambda arg: scale * kernel(scale * arg)) if antialiasing else kernel\n","    kernel_width *= 1.0 / scale if antialiasing else 1.0\n","\n","    # These are the coordinates of the output image\n","    out_coordinates = np.arange(1, out_length+1)\n","\n","    # These are the matching positions of the output-coordinates on the input image coordinates.\n","    # Best explained by example: say we have 4 horizontal pixels for HR and we downscale by SF=2 and get 2 pixels:\n","    # [1,2,3,4] -> [1,2]. Remember each pixel number is the middle of the pixel.\n","    # The scaling is done between the distances and not pixel numbers (the right boundary of pixel 4 is transformed to\n","    # the right boundary of pixel 2. pixel 1 in the small image matches the boundary between pixels 1 and 2 in the big\n","    # one and not to pixel 2. This means the position is not just multiplication of the old pos by scale-factor).\n","    # So if we measure distance from the left border, middle of pixel 1 is at distance d=0.5, border between 1 and 2 is\n","    # at d=1, and so on (d = p - 0.5).  we calculate (d_new = d_old / sf) which means:\n","    # (p_new-0.5 = (p_old-0.5) / sf)     ->          p_new = p_old/sf + 0.5 * (1-1/sf)\n","    match_coordinates = 1.0 * out_coordinates / scale + 0.5 * (1 - 1.0 / scale)\n","\n","    # This is the left boundary to start multiplying the filter from, it depends on the size of the filter\n","    left_boundary = np.floor(match_coordinates - kernel_width / 2)\n","\n","    # Kernel width needs to be enlarged because when covering has sub-pixel borders, it must 'see' the pixel centers\n","    # of the pixels it only covered a part from. So we add one pixel at each side to consider (weights can zeroize them)\n","    expanded_kernel_width = np.ceil(kernel_width) + 2\n","\n","    # Determine a set of field_of_view for each each output position, these are the pixels in the input image\n","    # that the pixel in the output image 'sees'. We get a matrix whos horizontal dim is the output pixels (big) and the\n","    # vertical dim is the pixels it 'sees' (kernel_size + 2)\n","    field_of_view = np.squeeze(np.uint(np.expand_dims(left_boundary, axis=1) + np.arange(expanded_kernel_width) - 1))\n","\n","    # Assign weight to each pixel in the field of view. A matrix whos horizontal dim is the output pixels and the\n","    # vertical dim is a list of weights matching to the pixel in the field of view (that are specified in\n","    # 'field_of_view')\n","    weights = fixed_kernel(1.0 * np.expand_dims(match_coordinates, axis=1) - field_of_view - 1)\n","\n","    # Normalize weights to sum up to 1. be careful from dividing by 0\n","    sum_weights = np.sum(weights, axis=1)\n","    sum_weights[sum_weights == 0] = 1.0\n","    weights = 1.0 * weights / np.expand_dims(sum_weights, axis=1)\n","\n","    # We use this mirror structure as a trick for reflection padding at the boundaries\n","    mirror = np.uint(np.concatenate((np.arange(in_length), np.arange(in_length - 1, -1, step=-1))))\n","    field_of_view = mirror[np.mod(field_of_view, mirror.shape[0])]\n","\n","    # Get rid of  weights and pixel positions that are of zero weight\n","    non_zero_out_pixels = np.nonzero(np.any(weights, axis=0))\n","    weights = np.squeeze(weights[:, non_zero_out_pixels])\n","    field_of_view = np.squeeze(field_of_view[:, non_zero_out_pixels])\n","\n","    # Final products are the relative positions and the matching weights, both are output_size X fixed_kernel_size\n","    return weights, field_of_view\n","\n","\n","def resize_along_dim(im, dim, weights, field_of_view):\n","    # To be able to act on each dim, we swap so that dim 0 is the wanted dim to resize\n","    tmp_im = np.swapaxes(im, dim, 0)\n","\n","    # We add singleton dimensions to the weight matrix so we can multiply it with the big tensor we get for\n","    # tmp_im[field_of_view.T], (bsxfun style)\n","    weights = np.reshape(weights.T, list(weights.T.shape) + (np.ndim(im) - 1) * [1])\n","\n","    # This is a bit of a complicated multiplication: tmp_im[field_of_view.T] is a tensor of order image_dims+1.\n","    # for each pixel in the output-image it matches the positions the influence it from the input image (along 1 dim\n","    # only, this is why it only adds 1 dim to the shape). We then multiply, for each pixel, its set of positions with\n","    # the matching set of weights. we do this by this big tensor element-wise multiplication (MATLAB bsxfun style:\n","    # matching dims are multiplied element-wise while singletons mean that the matching dim is all multiplied by the\n","    # same number\n","    tmp_out_im = np.sum(tmp_im[field_of_view.T] * weights, axis=0)\n","\n","    # Finally we swap back the axes to the original order\n","    return np.swapaxes(tmp_out_im, dim, 0)\n","\n","\n","def numeric_kernel(im, kernel, scale_factor, output_shape, kernel_shift_flag):\n","    # See kernel_shift function to understand what this is\n","    if kernel_shift_flag:\n","        kernel = kernel_shift(kernel, scale_factor)\n","\n","    # First run a correlation (convolution with flipped kernel)\n","    out_im = np.zeros_like(im)\n","    for channel in range(np.ndim(im)):\n","        out_im[:, :, channel] = filters.correlate(im[:, :, channel], kernel)\n","\n","    # Then subsample and return\n","    return out_im[np.round(np.linspace(0, im.shape[0] - 1 / scale_factor[0], output_shape[0])).astype(int)[:, None],\n","                  np.round(np.linspace(0, im.shape[1] - 1 / scale_factor[1], output_shape[1])).astype(int), :]\n","\n","\n","def kernel_shift(kernel, sf):\n","    # There are two reasons for shifting the kernel:\n","    # 1. Center of mass is not in the center of the kernel which creates ambiguity. There is no possible way to know\n","    #    the degradation process included shifting so we always assume center of mass is center of the kernel.\n","    # 2. We further shift kernel center so that top left result pixel corresponds to the middle of the sfXsf first\n","    #    pixels. Default is for odd size to be in the middle of the first pixel and for even sized kernel to be at the\n","    #    top left corner of the first pixel. that is why different shift size needed between od and even size.\n","    # Given that these two conditions are fulfilled, we are happy and aligned, the way to test it is as follows:\n","    # The input image, when interpolated (regular bicubic) is exactly aligned with ground truth.\n","\n","    # First calculate the current center of mass for the kernel\n","    current_center_of_mass = measurements.center_of_mass(kernel)\n","\n","    # The second (\"+ 0.5 * ....\") is for applying condition 2 from the comments above\n","    wanted_center_of_mass = np.array(kernel.shape) / 2 + 0.5 * (sf - (kernel.shape[0] % 2))\n","\n","    # Define the shift vector for the kernel shifting (x,y)\n","    shift_vec = wanted_center_of_mass - current_center_of_mass\n","\n","    # Before applying the shift, we first pad the kernel so that nothing is lost due to the shift\n","    # (biggest shift among dims + 1 for safety)\n","    kernel = np.pad(kernel, np.int(np.ceil(np.max(shift_vec))) + 1, 'constant')\n","\n","    # Finally shift the kernel and return\n","    return interpolation.shift(kernel, shift_vec)\n","\n","\n","# These next functions are all interpolation methods. x is the distance from the left pixel center\n","\n","\n","def cubic(x):\n","    absx = np.abs(x)\n","    absx2 = absx ** 2\n","    absx3 = absx ** 3\n","    return ((1.5*absx3 - 2.5*absx2 + 1) * (absx <= 1) +\n","            (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * ((1 < absx) & (absx <= 2)))\n","\n","\n","def lanczos2(x):\n","    return (((np.sin(pi*x) * np.sin(pi*x/2) + np.finfo(np.float32).eps) /\n","             ((pi**2 * x**2 / 2) + np.finfo(np.float32).eps))\n","            * (abs(x) < 2))\n","\n","\n","def box(x):\n","    return ((-0.5 <= x) & (x < 0.5)) * 1.0\n","\n","\n","def lanczos3(x):\n","    return (((np.sin(pi*x) * np.sin(pi*x/3) + np.finfo(np.float32).eps) /\n","            ((pi**2 * x**2 / 3) + np.finfo(np.float32).eps))\n","            * (abs(x) < 3))\n","\n","\n","def linear(x):\n","    return (x + 1) * ((-1 <= x) & (x < 0)) + (1 - x) * ((0 <= x) & (x <= 1))\n","\n","\n","def np_imresize(im, scale_factor=None, output_shape=None, kernel=None, antialiasing=True, kernel_shift_flag=False):\n","    return np.clip(imresize(im.transpose(1, 2, 0), scale_factor, output_shape, kernel, antialiasing,\n","                            kernel_shift_flag).transpose(2, 0, 1), 0, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEKiz_dOutAz","colab_type":"code","colab":{}},"source":["# Downsampler\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","class Downsampler(nn.Module):\n","    \"\"\"\n","        http://www.realitypixels.com/turk/computergraphics/ResamplingFilters.pdf\n","    \"\"\"\n","\n","    def __init__(self, n_planes, factor, kernel_type, phase=0, kernel_width=None, support=None, sigma=None,\n","                 preserve_size=False):\n","        \"\"\"\n","        :param n_planes:\n","        :param factor:\n","        :param kernel_type:\n","        :param float phase:\n","        :param kernel_width:\n","        :param support:\n","        :param sigma:\n","        :param preserve_size:\n","        \"\"\"\n","        super(Downsampler, self).__init__()\n","\n","        assert phase in [0, 0.5], 'phase should be 0 or 0.5'\n","\n","        if kernel_type == 'lanczos2':\n","            support = 2\n","            kernel_width = 4 * factor + 1\n","            kernel_type_ = 'lanczos'\n","\n","        elif kernel_type == 'lanczos3':\n","            support = 3\n","            kernel_width = 6 * factor + 1\n","            kernel_type_ = 'lanczos'\n","\n","        elif kernel_type == 'gauss12':\n","            kernel_width = 7\n","            sigma = 1 / 2\n","            kernel_type_ = 'gauss'\n","\n","        elif kernel_type == 'gauss1sq2':\n","            kernel_width = 9\n","            sigma = 1. / np.sqrt(2)\n","            kernel_type_ = 'gauss'\n","\n","        elif kernel_type in ['lanczos', 'gauss', 'box']:\n","            kernel_type_ = kernel_type\n","\n","        else:\n","            assert False, 'wrong name kernel'\n","\n","        # note that `kernel width` will be different to actual size for phase = 1/2\n","        self.kernel = get_kernel(factor, kernel_type_, phase, kernel_width, support=support, sigma=sigma)\n","\n","        downsampler = nn.Conv2d(n_planes, n_planes, kernel_size=self.kernel.shape, stride=factor, padding=0)\n","        downsampler.weight.data[:] = 0\n","        downsampler.bias.data[:] = 0\n","\n","        kernel_torch = torch.from_numpy(self.kernel)\n","        for i in range(n_planes):\n","            downsampler.weight.data[i, i] = kernel_torch\n","\n","        self.downsampler_ = downsampler\n","\n","        if preserve_size:\n","\n","            if self.kernel.shape[0] % 2 == 1:\n","                pad = int((self.kernel.shape[0] - 1) / 2.)\n","            else:\n","                pad = int((self.kernel.shape[0] - factor) / 2.)\n","\n","            self.padding = nn.ReplicationPad2d(pad)\n","\n","        self.preserve_size = preserve_size\n","\n","    def forward(self, input):\n","        if self.preserve_size:\n","            x = self.padding(input)\n","        else:\n","            x = input\n","        self.x = x\n","        return self.downsampler_(x)\n","\n","\n","def get_kernel(factor, kernel_type, phase, kernel_width, support=None, sigma=None):\n","    assert kernel_type in ['lanczos', 'gauss', 'box']\n","\n","    # factor  = float(factor)\n","    if phase == 0.5 and kernel_type != 'box':\n","        kernel = np.zeros([kernel_width - 1, kernel_width - 1])\n","    else:\n","        kernel = np.zeros([kernel_width, kernel_width])\n","\n","    if kernel_type == 'box':\n","        assert phase == 0.5, 'Box filter is always half-phased'\n","        kernel[:] = 1. / (kernel_width * kernel_width)\n","\n","    elif kernel_type == 'gauss':\n","        assert sigma, 'sigma is not specified'\n","        assert phase != 0.5, 'phase 1/2 for gauss not implemented'\n","\n","        center = (kernel_width + 1.) / 2.\n","        print(center, kernel_width)\n","        sigma_sq = sigma * sigma\n","\n","        for i in range(1, kernel.shape[0] + 1):\n","            for j in range(1, kernel.shape[1] + 1):\n","                di = (i - center) / 2.\n","                dj = (j - center) / 2.\n","                kernel[i - 1][j - 1] = np.exp(-(di * di + dj * dj) / (2 * sigma_sq))\n","                kernel[i - 1][j - 1] = kernel[i - 1][j - 1] / (2. * np.pi * sigma_sq)\n","    elif kernel_type == 'lanczos':\n","        assert support, 'support is not specified'\n","        center = (kernel_width + 1) / 2.\n","\n","        for i in range(1, kernel.shape[0] + 1):\n","            for j in range(1, kernel.shape[1] + 1):\n","\n","                if phase == 0.5:\n","                    di = abs(i + 0.5 - center) / factor\n","                    dj = abs(j + 0.5 - center) / factor\n","                else:\n","                    di = abs(i - center) / factor\n","                    dj = abs(j - center) / factor\n","\n","                pi_sq = np.pi * np.pi\n","\n","                val = 1\n","                if di != 0:\n","                    val = val * support * np.sin(np.pi * di) * np.sin(np.pi * di / support)\n","                    val = val / (np.pi * np.pi * di * di)\n","\n","                if dj != 0:\n","                    val = val * support * np.sin(np.pi * dj) * np.sin(np.pi * dj / support)\n","                    val = val / (np.pi * np.pi * dj * dj)\n","\n","                kernel[i - 1][j - 1] = val\n","\n","\n","    else:\n","        assert False, 'wrong method name'\n","\n","    kernel /= kernel.sum()\n","\n","    return kernel\n","\n","\n","def get_downsampled(image, downsample_factors):\n","    \"\"\"\n","    image is of type np.array\n","    downsampling_factor should be integer - e.g. 2 \n","    \"\"\"\n","    # TODO: move kernel type to args\n","    torch.backends.cudnn.enabled = True\n","    torch.backends.cudnn.benchmark = True\n","    data_type = torch.cuda.FloatTensor\n","    image_torch = np_to_torch(image).type(data_type)\n","    downsampled_images = [image_torch]\n","    for i in downsample_factors:\n","        downsampler = Downsampler(n_planes=image_torch.shape[1], factor=i,\n","                                  kernel_type='lanczos2', phase=0.5, preserve_size=True).cuda()\n","        downsampled_images.append(downsampler(image_torch))\n","\n","    return [torch_to_np(crop_torch_image(image, d=32)) for image in downsampled_images]\n","\n","\n","\n","def get_imresize_downsampled(image, downsampling_factor, downsampling_number):\n","    \"\"\"\n","    image is of type np.array\n","    downsampling_factor should be integer - e.g. 0.5\n","    \"\"\"\n","    # TODO: move kernel type to args\n","    downsampled_images = [image]\n","    for i in range(1, downsampling_number + 1):\n","        im = np.clip(imresize(image.transpose(1, 2, 0), scale_factor=(1 - (downsampling_factor * downsampling_number))).transpose(2,0,1), 0,1)\n","        downsampled_images.append(pil_to_np(crop_image(np_to_pil(im), d=32)))\n","\n","    return downsampled_images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT8NnHFPvNyX","colab_type":"code","colab":{}},"source":["# Layers.py\n","def weights_init(m):\n","    \"\"\" This is used to initialize weights of any network \"\"\"\n","    class_name = m.__class__.__name__\n","    if class_name.find('Conv') != -1:\n","        nn.init.xavier_normal(m.weight, 1.0)\n","        if hasattr(m.bias, 'data'):\n","            m.bias.data.fill_(0)\n","    elif class_name.find('BatchNorm2d') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","\n","class Ratio(nn.Module):\n","    def __init__(self):\n","        super(Ratio, self).__init__()\n","        self.multp = torch.autograd.Variable(torch.tensor(np.random.uniform(0, 1)),\n","                                             requires_grad=True).type(torch.cuda.FloatTensor)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self):\n","        return self.sig(self.multp)\n","\n","\n","class VectorRatio(nn.Module):\n","    def __init__(self, frames_number):\n","        super(VectorRatio, self).__init__()\n","        self.multp = torch.autograd.Variable(\n","            torch.tensor(np.random.uniform(0, 1, frames_number)).reshape([frames_number, 1, 1, 1])).type(torch.cuda.FloatTensor)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self):\n","        return self.sig(self.multp)\n","\n","\n","class Concat(nn.Module):\n","    def __init__(self, dim, *args):\n","        super(Concat, self).__init__()\n","        self.dim = dim\n","\n","        for idx, module_ in enumerate(args):\n","            self.add_module(str(idx), module_)\n","\n","    def forward(self, input_):\n","        inputs = []\n","        for module_ in self._modules.values():\n","            inputs.append(module_(input_))\n","\n","        inputs_shapes2 = [x.shape[2] for x in inputs]\n","        inputs_shapes3 = [x.shape[3] for x in inputs]\n","\n","        if np.all(np.array(inputs_shapes2) == min(inputs_shapes2)) and np.all(\n","                        np.array(inputs_shapes3) == min(inputs_shapes3)):\n","            inputs_ = inputs\n","        else:\n","            target_shape2 = min(inputs_shapes2)\n","            target_shape3 = min(inputs_shapes3)\n","\n","            inputs_ = []\n","            for inp in inputs:\n","                diff2 = (inp.size(2) - target_shape2) // 2\n","                diff3 = (inp.size(3) - target_shape3) // 2\n","                inputs_.append(inp[:, :, diff2: diff2 + target_shape2, diff3:diff3 + target_shape3])\n","\n","        return torch.cat(inputs_, dim=self.dim)\n","\n","    def __len__(self):\n","        return len(self._modules)\n","\n","\n","class GenNoise(nn.Module):\n","    def __init__(self, dim2):\n","        super(GenNoise, self).__init__()\n","        self.dim2 = dim2\n","\n","    def forward(self, x):\n","        a = list(x.size())\n","        a[1] = self.dim2\n","\n","        b = torch.zeros(a).type_as(x.data)\n","        b.normal_()\n","\n","        x = torch.autograd.Variable(b)\n","\n","        return x\n","\n","\n","class Swish(nn.Module):\n","    \"\"\"\n","        https://arxiv.org/abs/1710.05941\n","        The hype was so huge that I could not help but try it\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Swish, self).__init__()\n","        self.s = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return x * self.s(x)\n","\n","\n","def act(act_fun='LeakyReLU'):\n","    \"\"\"\n","        Either string defining an activation function or module (e.g. nn.ReLU)\n","    \"\"\"\n","    if isinstance(act_fun, str):\n","        if act_fun == 'LeakyReLU':\n","            return nn.LeakyReLU(0.2, inplace=True)\n","        elif act_fun == 'Swish':\n","            return Swish()\n","        elif act_fun == 'ELU':\n","            return nn.ELU()\n","        elif act_fun == 'none':\n","            return nn.Sequential()\n","        else:\n","            assert False\n","    else:\n","        return act_fun()\n","\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","def bn(num_features):\n","    return nn.BatchNorm2d(num_features)\n","\n","\n","def conv(in_f, out_f, kernel_size, stride=1, bias=True, pad='zero', downsample_mode='stride'):\n","    downsampler = None\n","    if stride != 1 and downsample_mode != 'stride':\n","\n","        if downsample_mode == 'avg':\n","            downsampler = nn.AvgPool2d(stride, stride)\n","        elif downsample_mode == 'max':\n","            downsampler = nn.MaxPool2d(stride, stride)\n","        elif downsample_mode in ['lanczos2', 'lanczos3']:\n","            downsampler = Downsampler(n_planes=out_f, factor=stride, kernel_type=downsample_mode, phase=0.5,\n","                                      preserve_size=True)\n","        stride = 1\n","\n","    padder = None\n","    to_pad = int((kernel_size - 1) / 2)\n","    if pad == 'reflection':\n","        padder = nn.ReflectionPad2d(to_pad)\n","        to_pad = 0\n","\n","    convolver = nn.Conv2d(in_f, out_f, kernel_size, stride, padding=to_pad, bias=bias)\n","\n","    layers = [x for x in [padder, convolver, downsampler] if x is not None]\n","    return nn.Sequential(*layers)\n","\n","\n","\n","\n","class FixedBlurLayer(nn.Module):\n","    def __init__(self, kernel):\n","        super(FixedBlurLayer, self).__init__()\n","        self.kernel = kernel\n","        to_pad_x = int((self.kernel.shape[0] - 1) / 2)\n","        to_pad_y = int((self.kernel.shape[1] - 1) / 2)\n","        self.pad = nn.ReflectionPad2d((to_pad_x, to_pad_x, to_pad_y, to_pad_y))\n","        self.mask_np = np.zeros(shape=(1, 3, self.kernel.shape[0], self.kernel.shape[1]))\n","        self.mask_np[0, 0, :, :] = self.kernel\n","        self.mask_np[0, 1, :, :] = self.kernel\n","        self.mask_np[0, 2, :, :] = self.kernel\n","        self.mask = nn.Parameter(data=torch.cuda.FloatTensor(self.mask_np), requires_grad=False)\n","\n","    def forward(self, x):\n","        return F.conv2d(self.pad(x), self.mask)\n","\n","\n","class VarianceLayer(nn.Module):\n","    # TODO: make it pad-able\n","    def __init__(self, patch_size=5, channels=1):\n","        self.patch_size = patch_size\n","        super(VarianceLayer, self).__init__()\n","        mean_mask = np.ones((channels, channels, patch_size, patch_size)) / (patch_size * patch_size)\n","        self.mean_mask = nn.Parameter(data=torch.cuda.FloatTensor(mean_mask), requires_grad=False)\n","        mask = np.zeros((channels, channels, patch_size, patch_size))\n","        mask[:, :, patch_size // 2, patch_size // 2] = 1.\n","        self.ones_mask = nn.Parameter(data=torch.cuda.FloatTensor(mask), requires_grad=False)\n","\n","    def forward(self, x):\n","        Ex_E = F.conv2d(x, self.ones_mask) - F.conv2d(x, self.mean_mask)\n","        return F.conv2d((Ex_E) ** 2, self.mean_mask)\n","\n","\n","class CovarianceLayer(nn.Module):\n","    def __init__(self, patch_size=5, channels=1):\n","        self.patch_size = patch_size\n","        super(CovarianceLayer, self).__init__()\n","        mean_mask = np.ones((channels, channels, patch_size, patch_size)) / (patch_size * patch_size)\n","        self.mean_mask = nn.Parameter(data=torch.cuda.FloatTensor(mean_mask), requires_grad=False)\n","        mask = np.zeros((channels, channels, patch_size, patch_size))\n","        mask[:, :, patch_size // 2, patch_size // 2] = 1.\n","        self.ones_mask = nn.Parameter(data=torch.cuda.FloatTensor(mask), requires_grad=False)\n","\n","    def forward(self, x, y):\n","        return F.conv2d((F.conv2d(x, self.ones_mask) - F.conv2d(x, self.mean_mask)) *\n","                        (F.conv2d(y, self.ones_mask) - F.conv2d(y, self.mean_mask)), self.mean_mask)\n","\n","class GrayscaleLayer(nn.Module):\n","    def __init__(self):\n","        super(GrayscaleLayer, self).__init__()\n","\n","    def forward(self, x):\n","        return torch.mean(x, 1, keepdim=True)\n","\n","\n","\n","def add_module(self, module_):\n","    self.add_module(str(len(self) + 1), module_)\n","\n","\n","torch.nn.Module.add = add_module"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUMR3zvEvY7i","colab_type":"code","colab":{}},"source":["# Losses.py\n","class StdLoss(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Loss on the variance of the image.\n","        Works in the grayscale.\n","        If the image is smooth, gets zero\n","        \"\"\"\n","        super(StdLoss, self).__init__()\n","        blur = (1 / 25) * np.ones((5, 5))\n","        blur = blur.reshape(1, 1, blur.shape[0], blur.shape[1])\n","        self.mse = nn.MSELoss()\n","        self.blur = nn.Parameter(data=torch.cuda.FloatTensor(blur), requires_grad=False)\n","        image = np.zeros((5, 5))\n","        image[2, 2] = 1\n","        image = image.reshape(1, 1, image.shape[0], image.shape[1])\n","        self.image = nn.Parameter(data=torch.cuda.FloatTensor(image), requires_grad=False)\n","        self.gray_scale = GrayscaleLayer()\n","\n","    def forward(self, x):\n","        x = self.gray_scale(x)\n","        return self.mse(torch.nn.functional.conv2d(x, self.image), torch.nn.functional.conv2d(x, self.blur))\n","\n","\n","class ExclusionLoss(nn.Module):\n","\n","    def __init__(self, level=3):\n","        \"\"\"\n","        Loss on the gradient. based on:\n","        http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single_Image_Reflection_CVPR_2018_paper.pdf\n","        \"\"\"\n","        super(ExclusionLoss, self).__init__()\n","        self.level = level\n","        self.avg_pool = torch.nn.AvgPool2d(2, stride=2).type(torch.cuda.FloatTensor)\n","        self.sigmoid = nn.Sigmoid().type(torch.cuda.FloatTensor)\n","\n","    def get_gradients(self, img1, img2):\n","        gradx_loss = []\n","        grady_loss = []\n","\n","        for l in range(self.level):\n","            gradx1, grady1 = self.compute_gradient(img1)\n","            gradx2, grady2 = self.compute_gradient(img2)\n","            # alphax = 2.0 * torch.mean(torch.abs(gradx1)) / torch.mean(torch.abs(gradx2))\n","            # alphay = 2.0 * torch.mean(torch.abs(grady1)) / torch.mean(torch.abs(grady2))\n","            alphay = 1\n","            alphax = 1\n","            gradx1_s = (self.sigmoid(gradx1) * 2) - 1\n","            grady1_s = (self.sigmoid(grady1) * 2) - 1\n","            gradx2_s = (self.sigmoid(gradx2 * alphax) * 2) - 1\n","            grady2_s = (self.sigmoid(grady2 * alphay) * 2) - 1\n","\n","            # gradx_loss.append(torch.mean(((gradx1_s ** 2) * (gradx2_s ** 2))) ** 0.25)\n","            # grady_loss.append(torch.mean(((grady1_s ** 2) * (grady2_s ** 2))) ** 0.25)\n","            gradx_loss += self._all_comb(gradx1_s, gradx2_s)\n","            grady_loss += self._all_comb(grady1_s, grady2_s)\n","            img1 = self.avg_pool(img1)\n","            img2 = self.avg_pool(img2)\n","        return gradx_loss, grady_loss\n","\n","    def _all_comb(self, grad1_s, grad2_s):\n","        v = []\n","        for i in range(3):\n","            for j in range(3):\n","                v.append(torch.mean(((grad1_s[:, j, :, :] ** 2) * (grad2_s[:, i, :, :] ** 2))) ** 0.25)\n","        return v\n","\n","    def forward(self, img1, img2):\n","        gradx_loss, grady_loss = self.get_gradients(img1, img2)\n","        loss_gradxy = sum(gradx_loss) / (self.level * 9) + sum(grady_loss) / (self.level * 9)\n","        return loss_gradxy / 2.0\n","\n","    def compute_gradient(self, img):\n","        gradx = img[:, :, 1:, :] - img[:, :, :-1, :]\n","        grady = img[:, :, :, 1:] - img[:, :, :, :-1]\n","        return gradx, grady\n","\n","\n","class ExtendedL1Loss(nn.Module):\n","    \"\"\"\n","    also pays attention to the mask, to be relative to its size\n","    \"\"\"\n","    def __init__(self):\n","        super(ExtendedL1Loss, self).__init__()\n","        self.l1 = nn.L1Loss().cuda()\n","\n","    def forward(self, a, b, mask):\n","        normalizer = self.l1(mask, torch.zeros(mask.shape).cuda())\n","        # if normalizer < 0.1:\n","        #     normalizer = 0.1\n","        c = self.l1(mask * a, mask * b) / normalizer\n","        return c\n","\n","\n","class NonBlurryLoss(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Loss on the distance to 0.5\n","        \"\"\"\n","        super(NonBlurryLoss, self).__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, x):\n","        return 1 - self.mse(x, torch.ones_like(x) * 0.5)\n","\n","\n","class GrayscaleLoss(nn.Module):\n","    def __init__(self):\n","        super(GrayscaleLoss, self).__init__()\n","        self.gray_scale = GrayscaleLayer()\n","        self.mse = nn.MSELoss().cuda()\n","\n","    def forward(self, x, y):\n","        x_g = self.gray_scale(x)\n","        y_g = self.gray_scale(y)\n","        return self.mse(x_g, y_g)\n","\n","\n","class GrayLoss(nn.Module):\n","    def __init__(self):\n","        super(GrayLoss, self).__init__()\n","        self.l1 = nn.L1Loss().cuda()\n","\n","    def forward(self, x):\n","        y = torch.ones_like(x) / 2.\n","        return 1 / self.l1(x, y)\n","\n","\n","class GradientLoss(nn.Module):\n","    \"\"\"\n","    L1 loss on the gradient of the picture\n","    \"\"\"\n","    def __init__(self):\n","        super(GradientLoss, self).__init__()\n","\n","    def forward(self, a):\n","        gradient_a_x = torch.abs(a[:, :, :, :-1] - a[:, :, :, 1:])\n","        gradient_a_y = torch.abs(a[:, :, :-1, :] - a[:, :, 1:, :])\n","        return torch.mean(gradient_a_x) + torch.mean(gradient_a_y)\n","\n","\n","class YIQGNGCLoss(nn.Module):\n","    def __init__(self, shape=5):\n","        super(YIQGNGCLoss, self).__init__()\n","        self.shape = shape\n","        self.var = VarianceLayer(self.shape, channels=1)\n","        self.covar = CovarianceLayer(self.shape, channels=1)\n","\n","    def forward(self, x, y):\n","        if x.shape[1] == 3:\n","            x_g = rgb_to_yiq(x)[:, :1, :, :]  # take the Y part\n","            y_g = rgb_to_yiq(y)[:, :1, :, :]  # take the Y part\n","        else:\n","            assert x.shape[1] == 1\n","            x_g = x  # take the Y part\n","            y_g = y  # take the Y part\n","        c = torch.mean(self.covar(x_g, y_g) ** 2)\n","        vv = torch.mean(self.var(x_g) * self.var(y_g))\n","        return c / vv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQX5OX-4vl-Y","colab_type":"code","colab":{}},"source":["# Noise.py\n","def get_noise(input_depth, method, spatial_size, noise_type='u', var=1. / 100):\n","    \"\"\"\n","    Returns a pytorch.Tensor of size (1 x `input_depth` x `spatial_size[0]` x `spatial_size[1]`)\n","    initialized in a specific way.\n","    Args:\n","        input_depth: number of channels in the tensor\n","        method: `noise` for fillting tensor with noise; `meshgrid` for np.meshgrid\n","        spatial_size: spatial size of the tensor to initialize\n","        noise_type: 'u' for uniform; 'n' for normal\n","        var: a factor, a noise will be multiplicated by. Basically it is standard deviation scaler.\n","    \"\"\"\n","    if isinstance(spatial_size, int):\n","        spatial_size = (spatial_size, spatial_size)\n","    if method == 'noise':\n","        shape = [1, input_depth, spatial_size[0], spatial_size[1]]\n","        net_input = torch.zeros(shape)\n","\n","        fill_noise(net_input, noise_type)\n","        net_input *= var\n","    elif method == 'meshgrid':\n","        assert input_depth % 2 == 0\n","        X, Y = np.meshgrid(np.arange(0, spatial_size[1]) / float(spatial_size[1] - 1),\n","                           np.arange(0, spatial_size[0]) / float(spatial_size[0] - 1))\n","        meshgrid = np.concatenate([X[None, :], Y[None, :]] * (input_depth // 2))\n","        net_input = np_to_torch(meshgrid)\n","    else:\n","        assert False\n","\n","    return net_input\n","\n","\n","def get_video_noise(input_depth, method, temporal_size, spatial_size, noise_type='u', var=1. / 100, type=\"dependant\"):\n","    \"\"\"\n","    Returns a pytorch.Tensor of size (frame_number x `input_depth` x `spatial_size[0]` x `spatial_size[1]`)\n","    initialized in a specific way.\n","    Args:\n","        input_depth: number of channels in the tensor\n","        method: `noise` for fillting tensor with noise; `meshgrid` for np.meshgrid\n","        temporal_size: number of frames\n","        spatial_size: spatial size of the tensor to initialize\n","        noise_type: 'u' for uniform; 'n' for normal\n","        var: a factor, a noise will be multiplicated by. Basically it is standard deviation scaler.\n","    \"\"\"\n","    if isinstance(spatial_size, int):\n","        spatial_size = (spatial_size, spatial_size)\n","    if method == 'noise':\n","        all_noise = []\n","        for i in range(temporal_size):\n","            shape = [input_depth, spatial_size[0], spatial_size[1]]\n","            if len(all_noise) > 0:\n","                if type == \"dependant\":\n","                    frame = np.random.uniform(0, 1, size=shape)\n","                    frame *= var\n","                    all_noise.append(all_noise[-1] + frame)\n","                elif type == \"half_dependant\":\n","                    frame = np.random.uniform(0, 1, size=shape)\n","                    frame *= var\n","                    new_noise = (all_noise[-1] + frame)\n","                    new_noise[:input_depth // 2,:,:] = (var * 10) * np.random.uniform(0, 1, size=shape)[:input_depth // 2,:,:]\n","                    all_noise.append(new_noise)\n","            else:\n","                frame = np.random.uniform(-0.5, 0.5, size=shape)\n","                frame *= (var * 10)\n","                all_noise.append(frame)\n","        return np_to_torch(np.array(all_noise))[0]\n","    elif method == 'meshgrid':\n","        assert False\n","        assert input_depth % 2 == 0\n","        X, Y = np.meshgrid(np.arange(0, spatial_size[1]) / float(spatial_size[1] - 1),\n","                           np.arange(0, spatial_size[0]) / float(spatial_size[0] - 1))\n","        meshgrid = np.concatenate([X[None, :], Y[None, :]] * (input_depth // 2))\n","        net_input = np_to_torch(meshgrid)\n","    else:\n","        assert False\n","\n","    return net_input\n","\n","\n","class NoiseNet(nn.Module):\n","    def __init__(self, channels=3, kernel_size=5):\n","        super(NoiseNet, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.channels = channels\n","        to_pad = int((self.kernel_size - 1) / 2)\n","        self.padder = nn.ReflectionPad2d(to_pad).type(torch.cuda.FloatTensor)\n","        to_pad = 0\n","        self.convolver = nn.Conv2d(channels, channels, self.kernel_size, 1, padding=to_pad, bias=True).type(torch.cuda.FloatTensor)\n","\n","    def forward(self, x):\n","        assert x.shape[1] == self.channels, (x.shape, self.channels)\n","        first = F.relu(self.convolver(self.padder(x)))\n","        second = F.relu(self.convolver(self.padder(first)))\n","        third = F.relu(self.convolver(self.padder(second)))\n","        assert x.shape == third.shape, (x.shape, third.shape)\n","        return third\n","\n","\n","def fill_noise(x, noise_type):\n","    \"\"\"\n","    Fills tensor `x` with noise of type `noise_type`.\n","    \"\"\"\n","    if noise_type == 'u':\n","        x.uniform_(-0.5, 0.5)\n","    elif noise_type == 'n':\n","        x.normal_()\n","    else:\n","        assert False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFY2asJ_vzvc","colab_type":"code","colab":{}},"source":["# Optimization.py\n","def optimize(optimizer_type, parameters, optimization_closure,\n","             plot_closure,\n","             learning_rate,\n","             num_iter,\n","             optimization_closure_args,\n","             plot_closure_args):\n","    \"\"\"\n","    Runs optimization loop.\n","    :param optimizer_type: 'LBFGS' of 'adam'\n","    :param parameters: list of Tensors to optimize over\n","    :param optimization_closure: function, that returns loss variable\n","    :param plot_closure: function that plots the loss and other information\n","    :param learning_rate: learning rate\n","    :param num_iter: number of iterations\n","    :param dict optimization_closure_args: the arguments for the optimization closure\n","    :param dict plot_closure_args: the arguments for the plot closure\n","    :return:\n","    \"\"\"\n","    if optimizer_type == 'LBFGS':\n","        assert False\n","\n","    elif optimizer_type == 'adam':\n","        print('Starting optimization with ADAM')\n","        optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n","\n","        for j in range(num_iter):\n","            optimizer.zero_grad()\n","            optimization_results = optimization_closure(j, **optimization_closure_args)\n","            if plot_closure:\n","                plot_closure(j, *optimization_results, **plot_closure_args)\n","            optimizer.step()\n","    else:\n","        assert False\n","\n","\n","def uneven_optimize(optimizer_type, parameters, optimization_closure, \n","                    plot_closure,\n","                    learning_rate,\n","                    num_iter, step,\n","                    optimization_closure_args,\n","                    plot_closure_args):\n","    \"\"\"\n","    Runs optimization loop.\n","    :param optimizer_type: 'LBFGS' of 'adam'\n","    :param parameters: list of Tensors to optimize over\n","    :param optimization_closure: function, that returns loss variable\n","    :param plot_closure: function that plots the loss and other information\n","    :param learning_rate: learning rate\n","    :param num_iter: number of iterations\n","    :param dict optimization_closure_args: the arguments for the optimization closure\n","    :param dict plot_closure_args: the arguments for the plot closure\n","    :return:\n","    \"\"\"\n","    if optimizer_type == 'LBFGS':\n","        assert False\n","\n","    elif optimizer_type == 'adam':\n","        print('Starting optimization with ADAM')\n","        next_step_optimization_args = None\n","        for j in range(num_iter // step):\n","            optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n","            for i in range(step):\n","                optimizer.zero_grad()\n","                optimization_results, next_step_optimization_args_temp = \\\n","                    optimization_closure(j*step + i, next_step_optimization_args, **optimization_closure_args)\n","                if plot_closure:\n","                    plot_closure(j*step + i, *optimization_results, **plot_closure_args)\n","                optimizer.step()\n","                if next_step_optimization_args is None:\n","                    # step zero\n","                    next_step_optimization_args = next_step_optimization_args_temp\n","            next_step_optimization_args = next_step_optimization_args_temp\n","    else:\n","        assert False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnGzU43Mv7R_","colab_type":"code","colab":{}},"source":["# skip_model.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def skip(\n","        num_input_channels=2, num_output_channels=3, num_channels_down=[16, 32, 64, 128, 128],\n","        num_channels_up=[16, 32, 64, 128, 128],\n","        num_channels_skip=[4, 4, 4, 4, 4], filter_size_down=3,\n","        filter_size_up=3, filter_skip_size=1, need_sigmoid=True, need_bias=True,\n","        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU', need1x1_up=True):\n","    \"\"\"\n","    Assembles encoder-decoder with skip connections.\n","    Arguments:\n","        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n","        pad (string): zero|reflection (default: 'zero')\n","        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n","        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n","    \"\"\"\n","    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n","\n","    n_scales = len(num_channels_down)\n","\n","    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n","        upsample_mode = [upsample_mode] * n_scales\n","\n","    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n","        downsample_mode = [downsample_mode] * n_scales\n","\n","    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n","        filter_size_down = [filter_size_down] * n_scales\n","\n","    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n","        filter_size_up = [filter_size_up] * n_scales\n","\n","    last_scale = n_scales - 1\n","\n","    cur_depth = None\n","\n","    model = nn.Sequential()\n","    model_tmp = model\n","\n","    input_depth = num_input_channels\n","    for i in range(len(num_channels_down)):\n","\n","        deeper = nn.Sequential()\n","        skip = nn.Sequential()\n","\n","        if num_channels_skip[i] != 0:\n","            model_tmp.add(Concat(1, skip, deeper))\n","        else:\n","            model_tmp.add(deeper)\n","\n","        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n","\n","        if num_channels_skip[i] != 0:\n","            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n","            skip.add(bn(num_channels_skip[i]))\n","            skip.add(act(act_fun))\n","\n","        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n","                        downsample_mode=downsample_mode[i]))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper_main = nn.Sequential()\n","\n","        if i == len(num_channels_down) - 1:\n","            # The deepest\n","            k = num_channels_down[i]\n","        else:\n","            deeper.add(deeper_main)\n","            k = num_channels_up[i + 1]\n","\n","        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n","\n","        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n","        model_tmp.add(bn(num_channels_up[i]))\n","        # model_tmp.add(layer_norm(num_channels_up[i]))\n","        model_tmp.add(act(act_fun))\n","\n","        if need1x1_up:\n","            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n","            model_tmp.add(bn(num_channels_up[i]))\n","            # model_tmp.add(layer_norm(num_channels_up[i]))\n","            model_tmp.add(act(act_fun))\n","\n","        input_depth = num_channels_down[i]\n","        model_tmp = deeper_main\n","\n","    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n","    if need_sigmoid:\n","        model.add(nn.Sigmoid())\n","    return model\n","\n","\n","def skip_mask(\n","        num_input_channels=2, num_output_channels=3,\n","        num_channels_down=[16, 32, 64, 128, 128], num_channels_up=[16, 32, 64, 128, 128],\n","        num_channels_skip=[4, 4, 4, 4, 4],\n","        filter_size_down=3, filter_size_up=3, filter_skip_size=1,\n","        need_sigmoid=True, need_bias=True,\n","        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU',\n","        need1x1_up=True):\n","    \"\"\"\n","    Assembles encoder-decoder with skip connections.\n","    Arguments:\n","        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n","        pad (string): zero|reflection (default: 'zero')\n","        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n","        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n","    \"\"\"\n","    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n","\n","    n_scales = len(num_channels_down)\n","\n","    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n","        upsample_mode = [upsample_mode] * n_scales\n","\n","    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n","        downsample_mode = [downsample_mode] * n_scales\n","\n","    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n","        filter_size_down = [filter_size_down] * n_scales\n","\n","    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n","        filter_size_up = [filter_size_up] * n_scales\n","\n","    last_scale = n_scales - 1\n","\n","    cur_depth = None\n","\n","    model = nn.Sequential()\n","    model_tmp = model\n","\n","    input_depth = num_input_channels\n","    for i in range(len(num_channels_down)):\n","\n","        deeper = nn.Sequential()\n","        skip = nn.Sequential()\n","\n","        if num_channels_skip[i] != 0:\n","            model_tmp.add(Concat(1, skip, deeper))\n","        else:\n","            model_tmp.add(deeper)\n","\n","        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n","\n","        if num_channels_skip[i] != 0:\n","            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n","            skip.add(bn(num_channels_skip[i]))\n","            skip.add(act(act_fun))\n","\n","        # skip.add(Concat(2, GenNoise(nums_noise[i]), skip_part))\n","\n","        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n","                        downsample_mode=downsample_mode[i]))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper_main = nn.Sequential()\n","\n","        if i == len(num_channels_down) - 1:\n","            # The deepest\n","            k = num_channels_down[i]\n","        else:\n","            deeper.add(deeper_main)\n","            k = num_channels_up[i + 1]\n","\n","        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n","\n","        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n","        model_tmp.add(bn(num_channels_up[i]))\n","        # model_tmp.add(layer_norm(num_channels_up[i]))\n","        model_tmp.add(act(act_fun))\n","\n","        if need1x1_up:\n","            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n","            model_tmp.add(bn(num_channels_up[i]))\n","            model_tmp.add(act(act_fun))\n","\n","        input_depth = num_channels_down[i]\n","        model_tmp = deeper_main\n","\n","    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n","    if need_sigmoid:\n","        model.add(nn.Sigmoid())\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5JZryipwHGx","colab_type":"code","colab":{}},"source":["# unet_model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class double_conv(nn.Module):\n","    '''(conv => BN => ReLU) * 2'''\n","    def __init__(self, in_ch, out_ch):\n","        super(double_conv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class inconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(inconv, self).__init__()\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class down(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(down, self).__init__()\n","        self.mpconv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            double_conv(in_ch, out_ch)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","        return x\n","\n","\n","class up(nn.Module):\n","    def __init__(self, in_ch, out_ch, bilinear=True):\n","        super(up, self).__init__()\n","\n","        #  would be a nice idea if the upsampling could be learned too,\n","        #  but my machine do not have enough memory to handle all those weights\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        diffX = x1.size()[2] - x2.size()[2]\n","        diffY = x1.size()[3] - x2.size()[3]\n","        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n","                        diffY // 2, int(diffY / 2)))\n","        x = torch.cat([x2, x1], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","\n","class outconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(outconv, self).__init__()\n","        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        self.inc = inconv(n_channels, 64)\n","        self.down1 = down(64, 128)\n","        self.down2 = down(128, 256)\n","        self.down3 = down(256, 512)\n","        self.down4 = down(512, 512)\n","        self.up1 = up(1024, 256)\n","        self.up2 = up(512, 128)\n","        self.up3 = up(256, 64)\n","        self.up4 = up(128, 64)\n","        self.outc = outconv(64, n_classes)\n","\n","    def forward(self, x):\n","        self.x1 = self.inc(x)\n","        self.x2 = self.down1(self.x1)\n","        self.x3 = self.down2(self.x2)\n","        self.x4 = self.down3(self.x3)\n","        self.x5 = self.down4(self.x4)\n","        self.x6 = self.up1(self.x5, self.x4)\n","        self.x7 = self.up2(self.x6, self.x3)\n","        self.x8 = self.up3(self.x7, self.x2)\n","        self.x9 = self.up4(self.x8, self.x1)\n","        self.y = self.outc(self.x9)\n","        return self.y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO26dUGCwNEU","colab_type":"code","colab":{}},"source":["# upsampler.py\n","from torch import nn\n","import torch\n","import numpy as np\n","\n","\n","class UpsamplerModel(nn.Module):\n","    def __init__(self, output_shape, factor):\n","        assert output_shape[0] % factor == 0\n","        assert output_shape[1] % factor == 0\n","        super(UpsamplerModel, self).__init__()\n","        self.output_shape = output_shape\n","        seed = np.ones((1, 1, output_shape[0] // factor, output_shape[1] // factor)) * 0.5\n","        self.sigmoid = nn.Sigmoid()\n","        self.seed = nn.Parameter(data=torch.cuda.FloatTensor(seed), requires_grad=True)\n","\n","    def forward(self):\n","        return nn.functional.interpolate(self.sigmoid(self.seed), size=self.output_shape, mode='bilinear')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSG0Jlg9dpZT","colab_type":"code","colab":{}},"source":["#skip_model.py\n","\n","def skip(\n","        num_input_channels=2, num_output_channels=3, num_channels_down=[16, 32, 64, 128, 128],\n","        num_channels_up=[16, 32, 64, 128, 128],\n","        num_channels_skip=[4, 4, 4, 4, 4], filter_size_down=3,\n","        filter_size_up=3, filter_skip_size=1, need_sigmoid=True, need_bias=True,\n","        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU', need1x1_up=True):\n","    \"\"\"\n","    Assembles encoder-decoder with skip connections.\n","\n","    Arguments:\n","        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n","        pad (string): zero|reflection (default: 'zero')\n","        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n","        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n","\n","    \"\"\"\n","    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n","\n","    n_scales = len(num_channels_down)\n","\n","    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n","        upsample_mode = [upsample_mode] * n_scales\n","\n","    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n","        downsample_mode = [downsample_mode] * n_scales\n","\n","    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n","        filter_size_down = [filter_size_down] * n_scales\n","\n","    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n","        filter_size_up = [filter_size_up] * n_scales\n","\n","    last_scale = n_scales - 1\n","\n","    cur_depth = None\n","\n","    model = nn.Sequential()\n","    model_tmp = model\n","\n","    input_depth = num_input_channels\n","    for i in range(len(num_channels_down)):\n","\n","        deeper = nn.Sequential()\n","        skip = nn.Sequential()\n","\n","        if num_channels_skip[i] != 0:\n","            model_tmp.add(Concat(1, skip, deeper))\n","        else:\n","            model_tmp.add(deeper)\n","\n","        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n","\n","        if num_channels_skip[i] != 0:\n","            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n","            skip.add(bn(num_channels_skip[i]))\n","            skip.add(act(act_fun))\n","\n","        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n","                        downsample_mode=downsample_mode[i]))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper_main = nn.Sequential()\n","\n","        if i == len(num_channels_down) - 1:\n","            # The deepest\n","            k = num_channels_down[i]\n","        else:\n","            deeper.add(deeper_main)\n","            k = num_channels_up[i + 1]\n","\n","        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n","\n","        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n","        model_tmp.add(bn(num_channels_up[i]))\n","        # model_tmp.add(layer_norm(num_channels_up[i]))\n","        model_tmp.add(act(act_fun))\n","\n","        if need1x1_up:\n","            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n","            model_tmp.add(bn(num_channels_up[i]))\n","            # model_tmp.add(layer_norm(num_channels_up[i]))\n","            model_tmp.add(act(act_fun))\n","\n","        input_depth = num_channels_down[i]\n","        model_tmp = deeper_main\n","\n","    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n","    if need_sigmoid:\n","        model.add(nn.Sigmoid())\n","    return model\n","\n","\n","def skip_mask(\n","        num_input_channels=2, num_output_channels=3,\n","        num_channels_down=[16, 32, 64, 128, 128], num_channels_up=[16, 32, 64, 128, 128],\n","        num_channels_skip=[4, 4, 4, 4, 4],\n","        filter_size_down=3, filter_size_up=3, filter_skip_size=1,\n","        need_sigmoid=True, need_bias=True,\n","        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU',\n","        need1x1_up=True):\n","    \"\"\"\n","    Assembles encoder-decoder with skip connections.\n","\n","    Arguments:\n","        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n","        pad (string): zero|reflection (default: 'zero')\n","        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n","        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n","\n","    \"\"\"\n","    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n","\n","    n_scales = len(num_channels_down)\n","\n","    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n","        upsample_mode = [upsample_mode] * n_scales\n","\n","    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n","        downsample_mode = [downsample_mode] * n_scales\n","\n","    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n","        filter_size_down = [filter_size_down] * n_scales\n","\n","    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n","        filter_size_up = [filter_size_up] * n_scales\n","\n","    last_scale = n_scales - 1\n","\n","    cur_depth = None\n","\n","    model = nn.Sequential()\n","    model_tmp = model\n","\n","    input_depth = num_input_channels\n","    for i in range(len(num_channels_down)):\n","\n","        deeper = nn.Sequential()\n","        skip = nn.Sequential()\n","\n","        if num_channels_skip[i] != 0:\n","            model_tmp.add(Concat(1, skip, deeper))\n","        else:\n","            model_tmp.add(deeper)\n","\n","        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n","\n","        if num_channels_skip[i] != 0:\n","            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n","            skip.add(bn(num_channels_skip[i]))\n","            skip.add(act(act_fun))\n","\n","        # skip.add(Concat(2, GenNoise(nums_noise[i]), skip_part))\n","\n","        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n","                        downsample_mode=downsample_mode[i]))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n","        deeper.add(bn(num_channels_down[i]))\n","        deeper.add(act(act_fun))\n","\n","        deeper_main = nn.Sequential()\n","\n","        if i == len(num_channels_down) - 1:\n","            # The deepest\n","            k = num_channels_down[i]\n","        else:\n","            deeper.add(deeper_main)\n","            k = num_channels_up[i + 1]\n","\n","        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n","\n","        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n","        model_tmp.add(bn(num_channels_up[i]))\n","        # model_tmp.add(layer_norm(num_channels_up[i]))\n","        model_tmp.add(act(act_fun))\n","\n","        if need1x1_up:\n","            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n","            model_tmp.add(bn(num_channels_up[i]))\n","            model_tmp.add(act(act_fun))\n","\n","        input_depth = num_channels_down[i]\n","        model_tmp = deeper_main\n","\n","    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n","    if need_sigmoid:\n","        model.add(nn.Sigmoid())\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlzU5R4qzvUI","colab_type":"code","colab":{}},"source":["#Select the images you want to use by uncommenting them\n","\n","#Horizontal text images\n","im1 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/text_hz_1.jpg')\n","im2 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/text_hz_2.jpg')\n","im3 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/text_hz_3.jpg')\n","\n","# #Original image watermarked with our own transparent watermark of double dip\n","\n","# im1 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/img_2/og_inc_1.jpg')\n","# im2 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/img_2/og_inc_2.jpg')\n","# im3 = prepare_image('/content/drive/My Drive/Colab Notebooks/images/img_2/og_inc_3.jpg')\n","\n","# #Original image watermarked with opaque bersion of watermark of double dip\n","# im1 = prepare_image('/content/drive/My Drive/Colab Notebooks/fotolia_inc_1.jpg')\n","# im2 = prepare_image('/content/drive/My Drive/Colab Notebooks/fotolia_inc_2.jpg')\n","# im3 = prepare_image('/content/drive/My Drive/Colab Notebooks/fotolia_inc_3.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgYlrQw8tUOQ","colab_type":"code","colab":{}},"source":["ManyImageWatermarkResult = namedtuple(\"ManyImageWatermarkResult\", ['cleans', 'mask', 'watermark', 'psnr'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"93VQ9ubKfALp","colab_type":"code","colab":{}},"source":["class ManyImagesWatermarkNoHint(object):\n","    def __init__(self, images_names, images, plot_during_training=True, num_iter_per_step=4000, num_step=1):\n","        self.images = images\n","        self.images_names = images_names\n","        self.plot_during_training = plot_during_training\n","        self.clean_nets = []\n","        self.watermark_net = None\n","        self.steps = num_step\n","        self.images_torch = None\n","        self.clean_nets_inputs = None\n","        self.clean_nets_outputs = None\n","        self.watermark_net_input = None\n","        self.watermark_net_output = None\n","        self.mask_net_input = None\n","        self.mask_net_output = None\n","        self.parameters = None\n","        self.blur_function = None\n","        self.num_iter_per_step = num_iter_per_step  # per step\n","        self.input_depth = 2\n","        self.multiscale_loss = None\n","        self.total_loss = None\n","        self.blur = None\n","        self.current_psnr = 0\n","        self.current_gradient = None\n","        self.current_result = None\n","        self.best_result = None\n","        self.learning_rate = 0.001\n","        self._init_all()\n","\n","    def _init_nets(self):\n","        pad = 'reflection'\n","        cleans = [skip(\n","            self.input_depth, 3,\n","            num_channels_down=[8, 16, 32, 64, 128],\n","            num_channels_up=[8, 16, 32, 64, 128],\n","            num_channels_skip=[0, 0, 0, 4, 4],\n","            upsample_mode='bilinear',\n","            filter_size_down=5,\n","            filter_size_up=5,\n","            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU') for _ in self.images]\n","\n","        self.clean_nets = [clean.type(torch.cuda.FloatTensor) for clean in cleans]\n","\n","        mask_net = skip(\n","            self.input_depth, 1,\n","            num_channels_down=[8, 16, 32, 64, 128],\n","            num_channels_up=[8, 16, 32, 64, 128],\n","            num_channels_skip=[0, 0, 0, 4, 4],\n","            upsample_mode='bilinear',\n","            filter_size_down=3,\n","            filter_size_up=3,\n","            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU')\n","\n","        self.mask_net = mask_net.type(torch.cuda.FloatTensor)\n","\n","        watermark = skip(\n","            self.input_depth, 3,\n","            num_channels_down=[8, 16, 32, 64, 128],\n","            num_channels_up=[8, 16, 32, 64, 128],\n","            num_channels_skip=[0, 0, 0, 4, 4],\n","            upsample_mode='bilinear',\n","            filter_size_down=3,\n","            filter_size_up=3,\n","            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU')\n","\n","        self.watermark_net = watermark.type(torch.cuda.FloatTensor)\n","\n","    def _init_images(self):\n","        # convention - first dim is all the images, second dim is the augmenations\n","        self.images_torch = [[np_to_torch(aug).type(torch.cuda.FloatTensor)\n","                              for aug in create_augmentations(image)] for image in self.images]\n","\n","    def _init_noise(self):\n","        input_type = 'noise'\n","        # self.left_net_inputs = self.images_torch\n","        self.clean_nets_inputs = []\n","        for image_idx in range(len(self.images)):\n","            original_noise = get_noise(self.input_depth, input_type,\n","                                                (self.images_torch[image_idx][0].shape[2],\n","                                                 self.images_torch[image_idx][0].shape[3])).type(torch.cuda.FloatTensor).detach()\n","            augmentations = create_augmentations(torch_to_np(original_noise))\n","            self.clean_nets_inputs.append([np_to_torch(aug).type(torch.cuda.FloatTensor).detach() for aug in augmentations])\n","\n","        original_noise = get_noise(self.input_depth, input_type,\n","                                  (self.images_torch[0][0].shape[2],\n","                                   self.images_torch[0][0].shape[3])).type(torch.cuda.FloatTensor).detach()\n","        augmentations = create_augmentations(torch_to_np(original_noise))\n","        self.mask_net_input = [np_to_torch(aug).type(torch.cuda.FloatTensor).detach() for aug in augmentations]\n","\n","        original_noise = get_noise(self.input_depth, input_type,\n","                                   (self.images_torch[0][0].shape[2],\n","                                    self.images_torch[0][0].shape[3])).type(torch.cuda.FloatTensor).detach()\n","        augmentations = create_augmentations(torch_to_np(original_noise))\n","        self.watermark_net_input = [np_to_torch(aug).type(torch.cuda.FloatTensor).detach() for aug in augmentations]\n","\n","\n","    def _init_parameters(self):\n","        self.parameters = sum([[p for p in clean_net.parameters()] for clean_net in self.clean_nets], []) + \\\n","                          [p for p in self.mask_net.parameters()] + \\\n","                          [p for p in self.watermark_net.parameters()]\n","\n","    def _init_losses(self):\n","        data_type = torch.cuda.FloatTensor\n","        self.l1_loss = nn.L1Loss().type(data_type)\n","        self.extended_l1_loss = ExtendedL1Loss().type(data_type)\n","        self.blur_function = StdLoss().type(data_type)\n","        self.gradient_loss = GradientLoss().type(data_type)\n","        self.gray_loss = GrayLoss().type(data_type)\n","\n","    def _init_all(self):\n","        self._init_images()\n","        self._init_losses()\n","        self._init_nets()\n","        self._init_parameters()\n","        self._init_noise()\n","\n","    def optimize(self):\n","        torch.backends.cudnn.enabled = True\n","        torch.backends.cudnn.benchmark = True\n","        for step in range(self.steps):\n","            self._step_initialization_closure(step)\n","            optimizer = torch.optim.Adam(self.parameters, lr=self.learning_rate)\n","            for j in range(self.num_iter_per_step):\n","                optimizer.zero_grad()\n","                self._optimization_closure(j, step)\n","                if self.plot_during_training:\n","                    self._iteration_plot_closure(j, step)\n","                optimizer.step()\n","            self._update_result_closure(step)\n","            # self._step_plot_closure(step)\n","\n","    def finalize(self):\n","        for image_name, clean, image in zip(self.images_names, self.best_result.cleans, self.images):\n","            save_image(image_name + \"_watermark\", self.best_result.watermark)\n","            save_image(image_name + \"_mask\", self.best_result.mask)\n","            save_image(image_name + \"_obtained_mask\", self.best_result.mask * self.best_result.watermark)\n","            save_image(image_name + \"_clean\", clean)\n","            save_image(image_name + \"_original\", image)\n","\n","    def _update_result_closure(self, step):\n","        self.current_result = ManyImageWatermarkResult(cleans=[torch_to_np(c) for c in self.clean_nets_outputs],\n","                                                       watermark=torch_to_np(self.watermark_net_output),\n","                                                       mask=torch_to_np(self.mask_net_output),\n","                                                       psnr=self.current_psnr)\n","        if self.best_result is None or self.best_result.psnr <= self.current_result.psnr:\n","            self.best_result = self.current_result\n","\n","    def _step_initialization_closure(self, step):\n","        \"\"\"\n","        at each start of step, we apply this\n","        :param step:\n","        :return:\n","        \"\"\"\n","        # we updating the inputs to new noises\n","        # self._init_nets()\n","        # self._init_parameters()\n","        # self._init_noise()\n","        pass\n","\n","    def _get_augmentation(self, iteration):\n","        if iteration % 4 in [1, 2, 3]:\n","            return 0\n","        iteration //= 2\n","        return iteration % 8\n","\n","    def _optimization_closure(self, iteration, step):\n","        \"\"\"\n","        the real iteration is step * self.num_iter_per_step + iteration\n","        :param iteration:\n","        :param step:\n","        :return:\n","        \"\"\"\n","        aug = self._get_augmentation(iteration)\n","        if iteration == self.num_iter_per_step - 1:\n","            reg_noise_std = 0\n","            aug = 0\n","        else:\n","            reg_noise_std = (1 / 1000.) * (iteration // 400)\n","        # creates left_net_inputs and right_net_inputs by adding small noise\n","        clean_nets_inputs = [clean_net_input[aug] + (clean_net_input[aug].clone().normal_() * reg_noise_std)\n","                             for clean_net_input in self.clean_nets_inputs]\n","        watermark_net_input = self.watermark_net_input[aug] # + (self.watermark_net_input[aug].clone().normal_() * reg_noise_std)\n","        mask_net_input = self.mask_net_input[aug]\n","        # applies the nets\n","        self.clean_nets_outputs = [clean_net(clean_net_input) for clean_net, clean_net_input\n","                                   in zip(self.clean_nets, clean_nets_inputs)]\n","        self.watermark_net_output = self.watermark_net(watermark_net_input)\n","        self.mask_net_output = self.mask_net(mask_net_input)\n","        self.total_loss = 0\n","        self.blur = 0\n","\n","        self.total_loss += sum(self.l1_loss(self.watermark_net_output * self.mask_net_output +\n","                                            clean_net_output * (1 - self.mask_net_output), image_torch[aug])\n","                               for clean_net_output, image_torch in zip(self.clean_nets_outputs, self.images_torch))\n","        self.total_loss.backward(retain_graph=True)\n","\n","    def _iteration_plot_closure(self, iteration, step):\n","        if iteration % 32 == 0:\n","            clean_out_nps = [torch_to_np(clean_net_output) for clean_net_output in self.clean_nets_outputs]\n","            watermark_out_np = torch_to_np(self.watermark_net_output)\n","            mask_out_np = torch_to_np(self.mask_net_output)\n","            self.current_psnr = compare_psnr(self.images[0], clean_out_nps[0] * (1 - mask_out_np) +\n","                                             mask_out_np * watermark_out_np)\n","            print('Iteration {:5d} PSNR {:5f} '.format(iteration, self.current_psnr),\n","                      '\\r', end='')\n","\n","    def _step_plot_closure(self, step_number):\n","        \"\"\"\n","        runs at the end of each step\n","        :param step_number:\n","        :return:\n","        \"\"\"\n","        for image_name, image, clean_net_output in zip(self.images_names, self.images, self.clean_nets_outputs):\n","            plot_image_grid(image_name + \"_watermark_clean_{}\".format(step_number),\n","                            [np.clip(torch_to_np(self.watermark_net_output), 0, 1),\n","                             np.clip(torch_to_np(clean_net_output), 0, 1)])\n","            plot_image_grid(image_name + \"_learned_image_{}\".format(step_number),\n","                            [np.clip(torch_to_np(self.watermark_net_output) * torch_to_np(self.mask_net_output) +\n","                                     (1 - torch_to_np(self.mask_net_output)) * torch_to_np(clean_net_output),\n","                                     0, 1), image])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6cUQOatThgB","colab_type":"code","colab":{}},"source":["def remove_watermark_many_images(imgs_names, imgs, final_name, iters=3):\n","    results = []\n","    for img_name, original in zip(imgs_names, imgs):\n","        save_image(final_name + \"_{}_original\".format(img_name), original)\n","    for i in range(iters):\n","        s = ManyImagesWatermarkNoHint([name + \"_{}\".format(i) for name in imgs_names], imgs, plot_during_training=False)\n","        s.optimize()\n","        s.finalize()\n","        results.append(s.best_result)\n","    obtained_watermark = median([result.mask * result.watermark for result in results])\n","\n","    obtained_imgs = [median([result.cleans[i] for result in results]) for i in range(len(imgs))]\n","\n","    v = np.zeros_like(obtained_watermark)\n","    v[obtained_watermark < 0.1] = 1\n","    final_imgs = []\n","    for im, obt_im in zip(imgs, obtained_imgs):\n","        final_imgs.append(v * im + (1 - v) * obt_im)\n","    for img_name, final in zip(imgs_names, final_imgs):\n","        save_image(final_name + \"_{}_final\".format(img_name), final)\n","    obtained_watermark[obtained_watermark < 0.1] = 0\n","    save_image(final_name + \"_final_watermark\", obtained_watermark)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iItrVq02e-4K","colab_type":"code","colab":{}},"source":["remove_watermark_many_images(['f1', 'f2', 'f3'], [im1, im2, im3], \"op_text_4000\")"],"execution_count":0,"outputs":[]}]}